{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renamed - original\n",
    "# 2sec.h5 - audio_deepfake_detector.h5\n",
    "# 2sec_scaler - audio_deepfake_scaler.pkl\n",
    "# 10sec - deepfake_audio_detector.pkl\n",
    "# 10sec_scaler - feature_scaler.pkl\n",
    "# mfcc - audio_classification_model.h5\n",
    "# mfcc_scaler - scaler.pkl\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---------- MODEL 1 SETUP ----------\n",
    "MODEL1_PATH = \"../models/10sec.pkl\"\n",
    "SCALER1_PATH = \"../models/10sec_scaler.pkl\"\n",
    "SAMPLE_RATE_1 = 22050\n",
    "\n",
    "def extract_features_model1(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE_1)\n",
    "        features = {}\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        y_harmonic = librosa.effects.harmonic(y)\n",
    "        y_percussive = librosa.effects.percussive(y)\n",
    "        features['harmonic_mean'] = np.mean(y_harmonic)\n",
    "        features['harmonic_std'] = np.std(y_harmonic)\n",
    "        features['percussive_mean'] = np.mean(y_percussive)\n",
    "        features['percussive_std'] = np.std(y_percussive)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        features['rms_dynamic_range'] = np.max(rms) - np.min(rms)\n",
    "        return pd.DataFrame([features])\n",
    "    except Exception as e:\n",
    "        print(f\"[Model 1] Error processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_model1(file_path, model, scaler):\n",
    "    features = extract_features_model1(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    scaled = scaler.transform(features)\n",
    "    pred = model.predict(scaled)\n",
    "    return 'fake' if pred[0] == 1 else 'real'\n",
    "\n",
    "# ---------- MODEL 2 SETUP ----------\n",
    "MODEL2_PATH = \"../models/2sec.h5\"\n",
    "SCALER2_PATH = \"../models/2sec_scaler.pkl\"\n",
    "SAMPLE_RATE_2 = 16000\n",
    "\n",
    "def extract_features_model2(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE_2)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features = np.concatenate([\n",
    "            np.mean(mfccs.T, axis=0),\n",
    "            np.mean(chroma.T, axis=0),\n",
    "            np.mean(spectral_centroids.T, axis=0),\n",
    "            np.mean(zcr.T, axis=0),\n",
    "            np.mean(rms.T, axis=0)\n",
    "        ])\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"[Model 2] Error processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_model2(file_path, model, scaler):\n",
    "    features = extract_features_model2(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    scaled = scaler.transform([features])\n",
    "    prediction = model.predict(scaled)[0][0]\n",
    "    return 'fake' if prediction > 0.5 else 'real'\n",
    "\n",
    "# ---------- MODEL 3 SETUP ----------\n",
    "MODEL3_PATH = \"../models/mfcc.h5\"\n",
    "SCALER3_PATH = \"../models/mfcc_scaler.pkl\"\n",
    "\n",
    "def extract_features_model3(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        return np.mean(mfcc.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[Model 3] Error processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_model3(file_path, model, scaler):\n",
    "    features = extract_features_model3(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    scaled = scaler.transform([features])\n",
    "    pred = model.predict(scaled)\n",
    "    label = np.argmax(pred)\n",
    "    return 'fake' if label == 1 else 'real'\n",
    "\n",
    "# ---------- ENSEMBLE PREDICTOR ----------\n",
    "def load_models_and_scalers():\n",
    "    model1 = joblib.load(MODEL1_PATH)\n",
    "    scaler1 = joblib.load(SCALER1_PATH)\n",
    "\n",
    "    model2 = tf.keras.models.load_model(MODEL2_PATH)\n",
    "    scaler2 = joblib.load(SCALER2_PATH)\n",
    "\n",
    "    model3 = tf.keras.models.load_model(MODEL3_PATH)\n",
    "    scaler3 = joblib.load(SCALER3_PATH)\n",
    "\n",
    "    return model1, scaler1, model2, scaler2, model3, scaler3\n",
    "\n",
    "def ensemble_predict(file_path, model1, scaler1, model2, scaler2, model3, scaler3):\n",
    "    print(f\"\\nüîç Predicting for file: {file_path}\")\n",
    "    \n",
    "    predictions = []\n",
    "    predictions.append(predict_model1(file_path, model1, scaler1))\n",
    "    predictions.append(predict_model2(file_path, model2, scaler2))\n",
    "    predictions.append(predict_model3(file_path, model3, scaler3))\n",
    "\n",
    "    print(f\"Model Predictions: {predictions}\")\n",
    "\n",
    "    final_verdict = 'FAKE' if \"fake\" in predictions else 'REAL'\n",
    "    print(f\"\\nüü• FINAL VERDICT: {final_verdict}\" if final_verdict == 'FAKE' else f\"\\nüü© FINAL VERDICT: {final_verdict}\")\n",
    "    return final_verdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/Users/arjun.maniyani/Desktop/PESU/Class/DL/project/ai_audio_detection/venv/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Predicting for file: ./audio_files/fake.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Model Predictions: ['fake', 'fake', 'real']\n",
      "\n",
      "üü• FINAL VERDICT: FAKE\n"
     ]
    }
   ],
   "source": [
    "# First, load all models and scalers (do this once)\n",
    "model1, scaler1, model2, scaler2, model3, scaler3 = load_models_and_scalers()\n",
    "\n",
    "# Then call the ensemble_predict function with any audio file\n",
    "file_path = \"../audio_files/fake.wav\"\n",
    "result = ensemble_predict(file_path, model1, scaler1, model2, scaler2, model3, scaler3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
