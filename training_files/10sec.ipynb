{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "DATASET_PATH = \"for-norm\"\n",
    "CSV_FEATURES_PATH = \"../features/audio_features.csv\"\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 10.0\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract important audio features for synthetic voice detection\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, duration=DURATION, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Initialize feature dictionary\n",
    "        features = {}\n",
    "        \n",
    "        # Short-time Fourier Transform\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        \n",
    "        # 1. Spectral features (most important for synthetic voice detection)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        \n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        \n",
    "        # 2. Zero crossing rate (helps detect artificial smoothness)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        \n",
    "        # 3. Harmonic and percussive components (synthetic voices often lack natural harmonics)\n",
    "        y_harmonic = librosa.effects.harmonic(y)\n",
    "        y_percussive = librosa.effects.percussive(y)\n",
    "        \n",
    "        features['harmonic_mean'] = np.mean(y_harmonic)\n",
    "        features['harmonic_std'] = np.std(y_harmonic)\n",
    "        features['percussive_mean'] = np.mean(y_percussive)\n",
    "        features['percussive_std'] = np.std(y_percussive)\n",
    "        \n",
    "        # 4. RMS energy dynamics\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        features['rms_dynamic_range'] = np.max(rms) - np.min(rms)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_dataset():\n",
    "    \"\"\"Create dataset by extracting features from all audio files\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for label in ['fake', 'real']:\n",
    "        folder_path = os.path.join(DATASET_PATH, label)\n",
    "        print(f\"Processing {label} files...\")\n",
    "        \n",
    "        for i, filename in enumerate(os.listdir(folder_path)):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                features = extract_features(file_path)\n",
    "                \n",
    "                if features:\n",
    "                    features['label'] = 1 if label == 'fake' else 0\n",
    "                    data.append(features)\n",
    "                \n",
    "                # Print progress\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Processed {i+1} {label} files\")\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(CSV_FEATURES_PATH, index=False)\n",
    "    print(f\"Dataset created with {len(df)} samples. Saved to {CSV_FEATURES_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    create_dataset()\n",
    "    print(f\"Feature extraction completed in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from CSV...\n",
      "Loaded 10798 samples with 15 features each\n",
      "\n",
      "Training Random Forest classifier...\n",
      "\n",
      "Training completed in 0.47 seconds\n",
      "Accuracy: 0.9162\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.93      0.90      0.92      1080\n",
      "        fake       0.91      0.93      0.92      1080\n",
      "\n",
      "    accuracy                           0.92      2160\n",
      "   macro avg       0.92      0.92      0.92      2160\n",
      "weighted avg       0.92      0.92      0.92      2160\n",
      "\n",
      "\n",
      "Model saved to deepfake_audio_detector.pkl\n",
      "Scaler saved to feature_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "CSV_FEATURES_PATH = \"../features/audio_features.csv\"\n",
    "MODEL_PATH = \"../models/10sec.pkl\"\n",
    "SCALER_PATH = \"../models/10sec_scaler.pkl\"\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load features from CSV file\"\"\"\n",
    "    print(\"Loading features from CSV...\")\n",
    "    df = pd.read_csv(CSV_FEATURES_PATH)\n",
    "    print(f\"Loaded {len(df)} samples with {df.shape[1]-1} features each\")\n",
    "    return df\n",
    "\n",
    "def train_and_evaluate(df):\n",
    "    \"\"\"Train and evaluate a model on the extracted features\"\"\"\n",
    "    # Prepare data\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Random Forest classifier\n",
    "    print(\"\\nTraining Random Forest classifier...\")\n",
    "    start_time = time.time()\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5\n",
    "    )\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['real', 'fake'])\n",
    "    \n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save model and scaler\n",
    "    joblib.dump(rf, MODEL_PATH)\n",
    "    joblib.dump(scaler, SCALER_PATH)\n",
    "    print(f\"\\nModel saved to {MODEL_PATH}\")\n",
    "    print(f\"Scaler saved to {SCALER_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    train_and_evaluate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and scaler...\n",
      "Model loaded successfully!\n",
      "\n",
      "Prediction Results:\n",
      "File: mle.wav\n",
      "Prediction: real\n",
      "Confidence: 79.31%\n",
      "Fake Probability: 20.69%\n",
      "Real Probability: 79.31%\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"deepfake_audio_detector.pkl\"\n",
    "SCALER_PATH = \"feature_scaler.pkl\"\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 10.0  # Should match what you used during training\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract the same features used during training\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, duration=DURATION, sr=SAMPLE_RATE)\n",
    "        \n",
    "        # Initialize feature dictionary\n",
    "        features = {}\n",
    "        \n",
    "        # Short-time Fourier Transform\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        \n",
    "        # 1. Spectral features\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        \n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        \n",
    "        # 2. Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        \n",
    "        # 3. Harmonic and percussive components\n",
    "        y_harmonic = librosa.effects.harmonic(y)\n",
    "        y_percussive = librosa.effects.percussive(y)\n",
    "        \n",
    "        features['harmonic_mean'] = np.mean(y_harmonic)\n",
    "        features['harmonic_std'] = np.std(y_harmonic)\n",
    "        features['percussive_mean'] = np.mean(y_percussive)\n",
    "        features['percussive_std'] = np.std(y_percussive)\n",
    "        \n",
    "        # 4. RMS energy dynamics\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        features['rms_dynamic_range'] = np.max(rms) - np.min(rms)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def predict_audio(file_path, model, scaler):\n",
    "    \"\"\"Predict whether an audio file is real or fake\"\"\"\n",
    "    # Extract features\n",
    "    features = extract_features(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame (single row)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    \n",
    "    # Scale features using the saved scaler\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_scaled)\n",
    "    proba = model.predict_proba(features_scaled)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'file': os.path.basename(file_path),\n",
    "        'prediction': 'fake' if prediction[0] == 1 else 'real',\n",
    "        'confidence': float(np.max(proba)),\n",
    "        'fake_probability': float(proba[0][1]),\n",
    "        'real_probability': float(proba[0][0])\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load model and scaler\n",
    "    print(\"Loading model and scaler...\")\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Get input file from user\n",
    "    file_path = input(\"Enter path to audio file (.wav): \").strip()\n",
    "    \n",
    "    # Check file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"Error: File not found!\")\n",
    "        return\n",
    "    \n",
    "    # Make prediction\n",
    "    result = predict_audio(file_path, model, scaler)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\nPrediction Results:\")\n",
    "        print(f\"File: {result['file']}\")\n",
    "        print(f\"Prediction: {result['prediction']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "        print(f\"Fake Probability: {result['fake_probability']:.2%}\")\n",
    "        print(f\"Real Probability: {result['real_probability']:.2%}\")\n",
    "    else:\n",
    "        print(\"Failed to process the audio file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
