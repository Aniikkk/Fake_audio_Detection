{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce08160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/Users/arjun.maniyani/Desktop/PESU/Class/DL/project/ai_audio_detection/venv/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble models loaded successfully.\n",
      "Audio streaming started from file: ./audio_files/anish.wav\n",
      "Streaming file in 322 chunks...\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Model Predictions: ['real', 'fake', 'real']\n",
      "\n",
      "🟥 FINAL VERDICT: FAKE\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'fake', 'real']\n",
      "\n",
      "🟥 FINAL VERDICT: FAKE\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Model Predictions: ['real', 'fake', 'real']\n",
      "\n",
      "🟥 FINAL VERDICT: FAKE\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'fake', 'real']\n",
      "\n",
      "🟥 FINAL VERDICT: FAKE\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Model Predictions: ['real', 'fake', 'real']\n",
      "\n",
      "🟥 FINAL VERDICT: FAKE\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "File streaming completed, waiting for final processing...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Model Predictions: ['real', 'fake', 'real']\n",
      "\n",
      "🟥 FINAL VERDICT: FAKE\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "🔍 Predicting for file: temp_stream.wav\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Model Predictions: ['real', 'real', 'real']\n",
      "\n",
      "🟩 FINAL VERDICT: REAL\n",
      "\n",
      "==================================================\n",
      "FINAL ENSEMBLE PREDICTION RESULTS\n",
      "==================================================\n",
      "Total windows analyzed: 30\n",
      "Distribution of ensemble predictions: {'FAKE': 6, 'REAL': 24}\n",
      "\n",
      "FINAL VERDICT: This audio is most likely REAL\n",
      "(80.0% of the windows agreed on this verdict)\n",
      "==================================================\n",
      "Audio streaming stopped.\n",
      "\n",
      "==================================================\n",
      "FINAL ENSEMBLE PREDICTION RESULTS\n",
      "==================================================\n",
      "Total windows analyzed: 30\n",
      "Distribution of ensemble predictions: {'FAKE': 6, 'REAL': 24}\n",
      "\n",
      "FINAL VERDICT: This audio is most likely REAL\n",
      "(80.0% of the windows agreed on this verdict)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import joblib\n",
    "import pyaudio\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "import wave\n",
    "import soundfile as sf\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# MODEL 1 SETUP (10sec)\n",
    "MODEL1_PATH = \"../models/10sec.pkl\"\n",
    "SCALER1_PATH = \"../models/10sec_scaler.pkl\"\n",
    "SAMPLE_RATE_1 = 22050\n",
    "\n",
    "def extract_features_model1(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE_1)\n",
    "        features = {}\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
    "        features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
    "        features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
    "        features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
    "        features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        y_harmonic = librosa.effects.harmonic(y)\n",
    "        y_percussive = librosa.effects.percussive(y)\n",
    "        features['harmonic_mean'] = np.mean(y_harmonic)\n",
    "        features['harmonic_std'] = np.std(y_harmonic)\n",
    "        features['percussive_mean'] = np.mean(y_percussive)\n",
    "        features['percussive_std'] = np.std(y_percussive)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features['rms_mean'] = np.mean(rms)\n",
    "        features['rms_std'] = np.std(rms)\n",
    "        features['rms_dynamic_range'] = np.max(rms) - np.min(rms)\n",
    "        return pd.DataFrame([features])\n",
    "    except Exception as e:\n",
    "        print(f\"[Model 1] Error processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_model1(file_path, model, scaler):\n",
    "    features = extract_features_model1(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    scaled = scaler.transform(features)\n",
    "    pred = model.predict(scaled)\n",
    "    return 'fake' if pred[0] == 1 else 'real'\n",
    "\n",
    "# MODEL 2 SETUP (2sec)\n",
    "MODEL2_PATH = \"../models/2sec.h5\"\n",
    "SCALER2_PATH = \"../models/2sec_scaler.pkl\"\n",
    "SAMPLE_RATE_2 = 16000\n",
    "\n",
    "def extract_features_model2(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE_2)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        rms = librosa.feature.rms(y=y)\n",
    "        features = np.concatenate([\n",
    "            np.mean(mfccs.T, axis=0),\n",
    "            np.mean(chroma.T, axis=0),\n",
    "            np.mean(spectral_centroids.T, axis=0),\n",
    "            np.mean(zcr.T, axis=0),\n",
    "            np.mean(rms.T, axis=0)\n",
    "        ])\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"[Model 2] Error processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_model2(file_path, model, scaler):\n",
    "    features = extract_features_model2(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    scaled = scaler.transform([features])\n",
    "    prediction = model.predict(scaled)[0][0]\n",
    "    return 'fake' if prediction > 0.5 else 'real'\n",
    "\n",
    "# MODEL 3 SETUP (MFCC)\n",
    "MODEL3_PATH = \"../models/mfcc.h5\"\n",
    "SCALER3_PATH = \"../models/mfcc_scaler.pkl\"\n",
    "\n",
    "def extract_features_model3(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "        return np.mean(mfcc.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"[Model 3] Error processing: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_model3(file_path, model, scaler):\n",
    "    features = extract_features_model3(file_path)\n",
    "    if features is None:\n",
    "        return None\n",
    "    scaled = scaler.transform([features])\n",
    "    pred = model.predict(scaled)\n",
    "    label = np.argmax(pred)\n",
    "    return 'fake' if label == 1 else 'real'\n",
    "\n",
    "def load_models_and_scalers():\n",
    "    model1 = joblib.load(MODEL1_PATH)\n",
    "    scaler1 = joblib.load(SCALER1_PATH)\n",
    "\n",
    "    model2 = tf.keras.models.load_model(MODEL2_PATH)\n",
    "    scaler2 = joblib.load(SCALER2_PATH)\n",
    "\n",
    "    model3 = tf.keras.models.load_model(MODEL3_PATH)\n",
    "    scaler3 = joblib.load(SCALER3_PATH)\n",
    "\n",
    "    return model1, scaler1, model2, scaler2, model3, scaler3\n",
    "\n",
    "def ensemble_predict(file_path, model1, scaler1, model2, scaler2, model3, scaler3):\n",
    "    print(f\"\\n🔍 Predicting for file: {file_path}\")\n",
    "    \n",
    "    predictions = []\n",
    "    predictions.append(predict_model1(file_path, model1, scaler1))\n",
    "    predictions.append(predict_model2(file_path, model2, scaler2))\n",
    "    predictions.append(predict_model3(file_path, model3, scaler3))\n",
    "\n",
    "    print(f\"Model Predictions: {predictions}\")\n",
    "\n",
    "    # FAKE if any model predicts FAKE\n",
    "    final_verdict = 'FAKE' if \"fake\" in predictions else 'REAL'\n",
    "\n",
    "    if final_verdict == 'FAKE':\n",
    "        print(f\"\\n🟥 FINAL VERDICT: {final_verdict}\")\n",
    "    else:\n",
    "        print(f\"\\n🟩 FINAL VERDICT: {final_verdict}\")\n",
    "    \n",
    "    return final_verdict\n",
    "\n",
    "class StreamingAudioEnsembleClassifier:\n",
    "    def __init__(self, sample_rate=16000, chunk_size=1024, buffer_seconds=3):\n",
    "        # Audio parameters\n",
    "        self.sample_rate = sample_rate\n",
    "        self.chunk_size = chunk_size\n",
    "        self.buffer_seconds = buffer_seconds\n",
    "        self.buffer_size = self.sample_rate * self.buffer_seconds\n",
    "        \n",
    "        # Audio buffer for accumulating data\n",
    "        self.audio_buffer = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        \n",
    "        # For overlapping windows\n",
    "        self.overlap_factor = 0.5  # 50% overlap\n",
    "        self.step_size = int(self.buffer_size * (1 - self.overlap_factor))\n",
    "        \n",
    "        self.all_predictions = []  # ensemble verdicts for each window\n",
    "        self.prediction_complete = threading.Event()\n",
    "        \n",
    "        # Audio stream setup\n",
    "        self.audio_queue = queue.Queue()\n",
    "        self.stop_flag = False\n",
    "        \n",
    "        # Debug flag\n",
    "        self.debug = True\n",
    "        \n",
    "        # Preprocessing flags (if desired)\n",
    "        self.normalize_audio = True\n",
    "        self.apply_noise_reduction = True\n",
    "        \n",
    "        (self.model1, self.scaler1,\n",
    "         self.model2, self.scaler2,\n",
    "         self.model3, self.scaler3) = load_models_and_scalers()\n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"Ensemble models loaded successfully.\")\n",
    "    \n",
    "    def start_audio_stream(self, use_microphone=True, audio_file=None):\n",
    "        \"\"\"Start capturing audio from microphone or file in a separate thread\"\"\"\n",
    "        self.stop_flag = False\n",
    "        self.all_predictions = []\n",
    "        self.prediction_complete.clear()\n",
    "        \n",
    "        if use_microphone:\n",
    "            self._start_microphone_stream()\n",
    "        elif audio_file:\n",
    "            self._start_file_stream(audio_file)\n",
    "        else:\n",
    "            raise ValueError(\"Either use_microphone must be True or audio_file must be provided\")\n",
    "        \n",
    "        # Start processing thread\n",
    "        self.processing_thread = threading.Thread(target=self._process_audio)\n",
    "        self.processing_thread.daemon = True\n",
    "        self.processing_thread.start()\n",
    "        \n",
    "        if use_microphone:\n",
    "            print(\"Audio streaming started from microphone. Speak now...\")\n",
    "        else:\n",
    "            print(f\"Audio streaming started from file: {audio_file}\")\n",
    "    \n",
    "    def _start_microphone_stream(self):\n",
    "        \"\"\"Start streaming from microphone\"\"\"\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        device_info = self.p.get_default_input_device_info()\n",
    "        if self.debug:\n",
    "            print(f\"Using input device: {device_info['name']}\")\n",
    "        self.stream = self.p.open(\n",
    "            format=pyaudio.paFloat32,\n",
    "            channels=1,\n",
    "            rate=self.sample_rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.chunk_size,\n",
    "            stream_callback=self._audio_callback\n",
    "        )\n",
    "    \n",
    "    def _audio_callback(self, in_data):\n",
    "        \"\"\"Callback for microphone streaming\"\"\"\n",
    "        if not self.stop_flag:\n",
    "            audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "            audio_data = self._preprocess_audio(audio_data)\n",
    "            self.audio_queue.put(audio_data)\n",
    "        return (in_data, pyaudio.paContinue)\n",
    "    \n",
    "    def _preprocess_audio(self, audio_data):\n",
    "        \"\"\"Enhanced preprocessing for microphone input\"\"\"\n",
    "        audio_data = audio_data * 3.0\n",
    "        \n",
    "        # Normalize \n",
    "        if self.normalize_audio:\n",
    "            max_val = np.max(np.abs(audio_data))\n",
    "            if max_val > 0.01:\n",
    "                audio_data = audio_data / max_val\n",
    "        \n",
    "        # Better noise reduction with a dynamic threshold\n",
    "        if self.apply_noise_reduction:\n",
    "            noise_floor = np.percentile(np.abs(audio_data), 15) * 2.5\n",
    "            audio_data = np.where(np.abs(audio_data) < noise_floor, 0, audio_data)\n",
    "        \n",
    "        # Apply a simple high-pass filter to remove low-frequency noise\n",
    "        if len(audio_data) > 10:\n",
    "            b = [0.98, -0.98]  # Simple high-pass filter coefficients\n",
    "            a = [1, -0.97]\n",
    "            from scipy import signal\n",
    "            audio_data = signal.lfilter(b, a, audio_data)\n",
    "        \n",
    "        return audio_data\n",
    "\n",
    "    def _start_file_stream(self, audio_file):\n",
    "        \"\"\"Stream audio from a file in chunks to simulate real-time\"\"\"\n",
    "        self.file_thread = threading.Thread(\n",
    "            target=self._stream_from_file,\n",
    "            args=(audio_file,)\n",
    "        )\n",
    "        self.file_thread.daemon = True\n",
    "        self.file_thread.start()\n",
    "    \n",
    "    def _stream_from_file(self, audio_file):\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_file, sr=self.sample_rate)\n",
    "            if y.dtype != np.float32:\n",
    "                y = y.astype(np.float32)\n",
    "            if np.max(np.abs(y)) > 0:\n",
    "                y = y / np.max(np.abs(y))\n",
    "            total_samples = len(y)\n",
    "            chunks = total_samples // self.chunk_size\n",
    "            print(f\"Streaming file in {chunks} chunks...\")\n",
    "            \n",
    "            for i in range(chunks):\n",
    "                if self.stop_flag:\n",
    "                    break\n",
    "                start = i * self.chunk_size\n",
    "                end = start + self.chunk_size\n",
    "                chunk = y[start:end]\n",
    "                self.audio_queue.put(chunk)\n",
    "                time.sleep(self.chunk_size / self.sample_rate * 0.5)\n",
    "            \n",
    "            if not self.stop_flag and total_samples % self.chunk_size > 0:\n",
    "                start = chunks * self.chunk_size\n",
    "                chunk = y[start:]\n",
    "                if len(chunk) < self.chunk_size:\n",
    "                    chunk = np.pad(chunk, (0, self.chunk_size - len(chunk)))\n",
    "                self.audio_queue.put(chunk)\n",
    "            \n",
    "            print(\"File streaming completed, waiting for final processing...\")\n",
    "            time.sleep(self.buffer_seconds * 2)\n",
    "            self.prediction_complete.set()\n",
    "        except Exception as e:\n",
    "            print(f\"Error streaming from file: {e}\")\n",
    "            self.prediction_complete.set()\n",
    "    \n",
    "    def _process_audio(self):\n",
    "        \"\"\"Process incoming audio chunks and make ensemble predictions\"\"\"\n",
    "        prediction_count = 0\n",
    "        last_buffer_energy = 0\n",
    "        \n",
    "        while not self.stop_flag:\n",
    "            try:\n",
    "                try:\n",
    "                    audio_chunk = self.audio_queue.get(timeout=0.5)\n",
    "                except queue.Empty:\n",
    "                    if self.prediction_complete.is_set():\n",
    "                        break\n",
    "                    continue\n",
    "                \n",
    "                shift = min(len(audio_chunk), self.step_size)\n",
    "                self.audio_buffer = np.roll(self.audio_buffer, -shift)\n",
    "                self.audio_buffer[-shift:] = audio_chunk[:shift]\n",
    "                \n",
    "                current_energy = np.mean(np.square(self.audio_buffer))\n",
    "                if current_energy < 0.0001:\n",
    "                    if last_buffer_energy >= 0.0001 and self.debug:\n",
    "                        print(\"Silence detected, skipping ensemble prediction\")\n",
    "                    last_buffer_energy = current_energy\n",
    "                    continue\n",
    "                last_buffer_energy = current_energy\n",
    "                \n",
    "                prediction_count += 1\n",
    "\n",
    "                if prediction_count >= 10:\n",
    "                    self._ensemble_predict()\n",
    "                    prediction_count = 0\n",
    "                \n",
    "                time.sleep(0.01)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in audio processing: {e}\")\n",
    "        \n",
    "        if not self.stop_flag:\n",
    "            self._show_final_prediction()\n",
    "    \n",
    "    def _ensemble_predict(self):\n",
    "        \"\"\"Write current audio buffer to a temporary WAV file and run ensemble predictor\"\"\"\n",
    "        try:\n",
    "            temp_filename = \"temp_stream.wav\"\n",
    "\n",
    "            sf.write(temp_filename, self.audio_buffer, self.sample_rate)\n",
    "\n",
    "            verdict = ensemble_predict(temp_filename,\n",
    "                                       self.model1, self.scaler1,\n",
    "                                       self.model2, self.scaler2,\n",
    "                                       self.model3, self.scaler3)\n",
    "            self.all_predictions.append(verdict.upper())\n",
    "            os.remove(temp_filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during ensemble prediction: {e}\")\n",
    "    \n",
    "    def _show_final_prediction(self):\n",
    "        \"\"\"Aggregate and display the ensemble predictions obtained over the streaming period\"\"\"\n",
    "        if not self.all_predictions:\n",
    "            print(\"No ensemble predictions were made.\")\n",
    "            return\n",
    "        \n",
    "        # Majority voting aggregation\n",
    "        prediction_counts = Counter(self.all_predictions)\n",
    "        majority_prediction = prediction_counts.most_common(1)[0][0]\n",
    "        majority_percentage = (prediction_counts[majority_prediction] / len(self.all_predictions)) * 100\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"FINAL ENSEMBLE PREDICTION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Total windows analyzed: {len(self.all_predictions)}\")\n",
    "        print(f\"Distribution of ensemble predictions: {dict(prediction_counts)}\")\n",
    "        print(f\"\\nFINAL VERDICT: This audio is most likely {majority_prediction}\")\n",
    "        print(f\"({majority_percentage:.1f}% of the windows agreed on this verdict)\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    def stop_audio_stream(self):\n",
    "        \"\"\"Stop the audio stream and clean up resources\"\"\"\n",
    "        self.stop_flag = True\n",
    "        if hasattr(self, 'processing_thread'):\n",
    "            self.processing_thread.join(timeout=1)\n",
    "        if hasattr(self, 'file_thread') and getattr(self, 'file_thread', None) is not None:\n",
    "            self.file_thread.join(timeout=1)\n",
    "        if hasattr(self, 'stream'):\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "        if hasattr(self, 'p'):\n",
    "            self.p.terminate()\n",
    "        print(\"Audio streaming stopped.\")\n",
    "        self._show_final_prediction()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        classifier = StreamingAudioEnsembleClassifier(\n",
    "            sample_rate=16000,\n",
    "            chunk_size=1024,\n",
    "            buffer_seconds=3\n",
    "        )\n",
    "        \n",
    "        # Set to True to use microphone or provide an audio file path\n",
    "        use_microphone = False\n",
    "        audio_file = \"../audio_files/anish.wav\"\n",
    "        \n",
    "        if not use_microphone and not os.path.exists(audio_file):\n",
    "            print(f\"Error: Audio file '{audio_file}' not found.\")\n",
    "            exit(1)\n",
    "        \n",
    "        classifier.start_audio_stream(\n",
    "            use_microphone=use_microphone,\n",
    "            audio_file=audio_file if not use_microphone else None\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            if not use_microphone:\n",
    "                classifier.prediction_complete.wait()\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                recording_duration = 30\n",
    "                print(f\"Listening for {recording_duration} seconds (press Ctrl+C to stop)...\")\n",
    "                for i in range(recording_duration):\n",
    "                    if i % 5 == 0 and i > 0:\n",
    "                        print(f\"Listened for {i} seconds...\")\n",
    "                    time.sleep(1)\n",
    "                print(\"Recording complete, processing final results...\")\n",
    "                time.sleep(2)\n",
    "                classifier.prediction_complete.set()\n",
    "                time.sleep(1)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"User interrupt, stopping...\")\n",
    "        finally:\n",
    "            classifier.stop_audio_stream()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
